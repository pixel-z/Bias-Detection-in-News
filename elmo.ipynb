{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6DZqk4FR9-Ah"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import tarfile\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wFmayISrEoY",
        "outputId": "cccc0d19-f935-420d-a120-499292db9b0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2VfMYVAt11w",
        "outputId": "addfb717-6f9c-4681-9ac6-59dc23d6c1bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'2019101117_Shivang Gupta_transcript.png'   report.gdoc\n",
            "'aec-gcjx-ohs – 28 Sept 2021.gjam'\t    Resume.pdf\n",
            " allenmodel\t\t\t\t   'Resume-Shivang_gupta (1).pdf'\n",
            " checkpoints\t\t\t\t    Resume-Shivang_gupta.pdf\n",
            "'Colab Notebooks'\t\t\t    Screencastify\n",
            " dataset\t\t\t\t    swb\n",
            "'Dysarthria Database'\t\t\t    switchboard_word_alignments.tar.gz\n",
            "'Getting started.pdf'\t\t\t   'Untitled document (1).gdoc'\n",
            "'Humanizing voice assistants.gdoc'\t   'Untitled document (2).gdoc'\n",
            "'kzg-dxvw-ksj – 23 Apr 2021.gjam'\t   'Untitled document (3).gdoc'\n",
            " NLP_classes\t\t\t\t   'Untitled document.gdoc'\n"
          ]
        }
      ],
      "source": [
        "! ls \"/content/drive/MyDrive\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zIfqUANo1p0d"
      },
      "outputs": [],
      "source": [
        "os.mkdir(\"/content/drive/MyDrive/checkpoints\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8Rfm8jXBFRxP"
      },
      "outputs": [],
      "source": [
        "os.chdir(\"/content/drive/MyDrive/checkpoints\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpbVh0uYFWhJ",
        "outputId": "bad1f988-97d4-4617-adfe-ce1c0d8f3782"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2021-12-10 12:57:39--  https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway_tf_checkpoint/model.ckpt-935588.index\n",
            "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.201.168\n",
            "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.201.168|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3277 (3.2K) [binary/octet-stream]\n",
            "Saving to: ‘model.ckpt-935588.index’\n",
            "\n",
            "model.ckpt-935588.i 100%[===================>]   3.20K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2021-12-10 12:57:40 (3.30 MB/s) - ‘model.ckpt-935588.index’ saved [3277/3277]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway_tf_checkpoint/model.ckpt-935588.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sq1gjTn5FiGu",
        "outputId": "53335f86-fcd4-403e-a97a-799e7430167a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2021-12-10 12:57:45--  https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway_tf_checkpoint/model.ckpt-935588.data-00000-of-00001\n",
            "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.152.144\n",
            "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.152.144|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4005211776 (3.7G) [binary/octet-stream]\n",
            "Saving to: ‘model.ckpt-935588.data-00000-of-00001’\n",
            "\n",
            "model.ckpt-935588.d 100%[===================>]   3.73G  31.7MB/s    in 1m 58s  \n",
            "\n",
            "2021-12-10 12:59:43 (32.4 MB/s) - ‘model.ckpt-935588.data-00000-of-00001’ saved [4005211776/4005211776]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway_tf_checkpoint/model.ckpt-935588.data-00000-of-00001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3KlkMMXFmEk",
        "outputId": "cdf7d2f3-56bb-42ec-ddc9-8dc200907b19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2021-12-10 12:59:46--  https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway_tf_checkpoint/model.ckpt-935588.meta\n",
            "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.92.164.8\n",
            "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.92.164.8|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22121605 (21M) [binary/octet-stream]\n",
            "Saving to: ‘model.ckpt-935588.meta’\n",
            "\n",
            "model.ckpt-935588.m 100%[===================>]  21.10M  15.1MB/s    in 1.4s    \n",
            "\n",
            "2021-12-10 12:59:48 (15.1 MB/s) - ‘model.ckpt-935588.meta’ saved [22121605/22121605]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway_tf_checkpoint/model.ckpt-935588.meta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTJ-boNuFvFM",
        "outputId": "6887f355-b610-4eca-eb4f-1e479164b74d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2021-12-10 12:59:52--  https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/vocab-2016-09-10.txt\n",
            "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 3.5.79.179\n",
            "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|3.5.79.179|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7430437 (7.1M) [text/plain]\n",
            "Saving to: ‘vocab-2016-09-10.txt’\n",
            "\n",
            "vocab-2016-09-10.tx 100%[===================>]   7.09M  2.14MB/s    in 3.3s    \n",
            "\n",
            "2021-12-10 12:59:56 (2.14 MB/s) - ‘vocab-2016-09-10.txt’ saved [7430437/7430437]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/vocab-2016-09-10.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpbLfGjuFzDX",
        "outputId": "bdb2edc2-9f4c-4618-f17f-b72ea20016e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2021-12-10 12:59:59--  https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway_tf_checkpoint/checkpoint\n",
            "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.228.0\n",
            "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.228.0|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 91 [binary/octet-stream]\n",
            "Saving to: ‘checkpoint’\n",
            "\n",
            "checkpoint          100%[===================>]      91  --.-KB/s    in 0s      \n",
            "\n",
            "2021-12-10 12:59:59 (2.38 MB/s) - ‘checkpoint’ saved [91/91]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway_tf_checkpoint/checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DD8q7k3rF2RE",
        "outputId": "8b20098e-1664-42e0-d773-fa025482b7e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2021-12-10 13:00:02--  https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway_tf_checkpoint/options.json\n",
            "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.228.0\n",
            "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.228.0|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 568 [application/json]\n",
            "Saving to: ‘options.json’\n",
            "\n",
            "options.json        100%[===================>]     568  --.-KB/s    in 0s      \n",
            "\n",
            "2021-12-10 13:00:02 (14.5 MB/s) - ‘options.json’ saved [568/568]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway_tf_checkpoint/options.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zyktkPGLXvt",
        "outputId": "efeaadc8-c195-454e-eaaa-1ee82a149f5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 3940205\n",
            "-rw------- 1 root root         91 May  1  2018 checkpoint\n",
            "-rw------- 1 root root 4005211776 Dec 21  2017 model.ckpt-935588.data-00000-of-00001\n",
            "-rw------- 1 root root       3277 Dec 21  2017 model.ckpt-935588.index\n",
            "-rw------- 1 root root   22121605 Dec 21  2017 model.ckpt-935588.meta\n",
            "-rw------- 1 root root        568 Dec 21  2017 options.json\n",
            "-rw------- 1 root root    7430437 Apr 23  2018 vocab-2016-09-10.txt\n"
          ]
        }
      ],
      "source": [
        "# os.chdir(\"/content/drive/MyDrive/allenmodel\")\n",
        "! ls -al"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MRn5casarwmp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmceT0XbtPqm",
        "outputId": "325c00c4-d701-4398-9199-d2122f449ec0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hollywood will not easily recover from Harvey Weinstein -- not for a long time. The hypocrisy level has hit Defcon 1, 9.9 on the Richter scale.', \"Hollywood’s politics have always been a self-serving charade, a liberal masquerade for a rapacious and lubricious lifestyle. But now, thanks to the Weinstein scandal, we see it more clearly than ever. And it couldn't be more repellent. (I had always thought Bill Clinton would have made the greatest studio executive of all time. Now I'm convinced of it.)\", 'If conservative investors had any courage, this would be the time to make a hostile takeover of the movie business. Unfortunately, they don’t. I know this from bitter personal experience. Wealthy conservatives are delighted to support the Philharmonic, but when it comes to popular culture they turn away, as if afraid to get their hands dirty.', 'That this is a huge mistake should be obvious. They have abandoned the culture -- and our children -- to the creepiest people imaginable. What is going on in Hollywood is far from being just about Harvey. It’s approaching a pandemic. So many previously silent assaulted or raped women are coming out of the woodwork, it seems like a long-belated remake of “Cheaper by the Dozen.” No one knows who will be next or if it will stop at Harvey.', 'The rot is everywhere, even, perhaps especially, in the precincts of “high art.” Gwyneth Paltrow says now is the time to put an end to these attacks on women. But where was she years ago when Harvey got “handsy” with her? Looking the other way while earning millions and garnering Oscars. Meryl Streep claimed she was clueless about Weinstein’s repulsive antics. Time to award her her greatest Oscar yet -- for playing someone deaf, dumb, and blind while living as a troglodyte in the Gobi desert. Either the woman’s a liar or an utter nincompoop. I’ll go with the former.', 'As for the great feminist George Clooney -- the first male star out of the box to condemn Weinstein’s behavior -- let’s give him the Nobel Prize in virtue signaling. By coming forward, he was able to ace out his competition -- Howard Zinn-loving Matt Damon, who disgraced himself forever by covering up for Harvey a decade ago. (For those who may have missed it in the onslaught of sleazy details, Damon assured then New York Times reporter Sharon Waxman that Miramax’s high-paid Italian representative was a genuine “creative film executive” and not Harvey’s European procurer, as was, evidently correctly, rumored. Damon is the same “progressive” movie star who makes films opposing school choice for the masses while living in a thirty million dollar house and sending his kids to private school. I take it back -- maybe we should give him the Nobel in virtue signaling.)']\n"
          ]
        }
      ],
      "source": [
        "os.chdir(\"/content/drive/MyDrive\")\n",
        "xml_data = open('./dataset/articles-training-byarticle-20181122.xml', 'r').read()  # Read file\n",
        "root = ET.XML(xml_data)\n",
        "\n",
        "data = []\n",
        "\n",
        "for i, child in enumerate(root):\n",
        "    data.append([subchild.text for subchild in child])\n",
        "\n",
        "df = pd.DataFrame(data).T  # Write in DF and transpose it\n",
        "\n",
        "print(data[218])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fOXe0nEtVc3",
        "outputId": "019a59ea-6f20-41e0-c43e-34ede0f3e5ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n"
          ]
        }
      ],
      "source": [
        "# loading labels\n",
        "xml_data1 = open('./dataset/ground-truth-training-byarticle-20181122.xml', 'r').read()  # Read file\n",
        "root1 = ET.XML(xml_data1)\n",
        "\n",
        "labels = []\n",
        "\n",
        "for i, child in enumerate(root1):\n",
        "    if child.attrib['hyperpartisan']=='true':\n",
        "        labels.append(1)\n",
        "    else:\n",
        "        labels.append(0)\n",
        "\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpS0KzgntcXn",
        "outputId": "fece0bd7-677a-477c-a5ac-e26536d3cfd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0EJ7S9lkteW7"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1EAao8CNcp7E"
      },
      "outputs": [],
      "source": [
        "os.chdir(\"/content/drive/MyDrive/allenmodel\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eg9dYQ5ztgsg",
        "outputId": "304c0a9d-0f96-414f-e19b-d87c25a37da6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing bilm.egg-info/PKG-INFO\n",
            "writing dependency_links to bilm.egg-info/dependency_links.txt\n",
            "writing requirements to bilm.egg-info/requires.txt\n",
            "writing top-level names to bilm.egg-info/top_level.txt\n",
            "reading manifest file 'bilm.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'bilm.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/bilm\n",
            "copying build/lib/bilm/elmo.py -> build/bdist.linux-x86_64/egg/bilm\n",
            "copying build/lib/bilm/training.py -> build/bdist.linux-x86_64/egg/bilm\n",
            "copying build/lib/bilm/data.py -> build/bdist.linux-x86_64/egg/bilm\n",
            "copying build/lib/bilm/model.py -> build/bdist.linux-x86_64/egg/bilm\n",
            "copying build/lib/bilm/__init__.py -> build/bdist.linux-x86_64/egg/bilm\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bilm/elmo.py to elmo.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bilm/training.py to training.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bilm/data.py to data.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bilm/model.py to model.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bilm/__init__.py to __init__.cpython-37.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying bilm.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying bilm.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying bilm.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying bilm.egg-info/not-zip-safe -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying bilm.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying bilm.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "creating 'dist/bilm-0.1.post5-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing bilm-0.1.post5-py3.7.egg\n",
            "creating /usr/local/lib/python3.7/dist-packages/bilm-0.1.post5-py3.7.egg\n",
            "Extracting bilm-0.1.post5-py3.7.egg to /usr/local/lib/python3.7/dist-packages\n",
            "Adding bilm 0.1.post5 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.7/dist-packages/bilm-0.1.post5-py3.7.egg\n",
            "Processing dependencies for bilm==0.1.post5\n",
            "Searching for h5py==3.1.0\n",
            "Best match: h5py 3.1.0\n",
            "Adding h5py 3.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for cached-property==1.5.2\n",
            "Best match: cached-property 1.5.2\n",
            "Adding cached-property 1.5.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for numpy==1.19.5\n",
            "Best match: numpy 1.19.5\n",
            "Adding numpy 1.19.5 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.7 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Finished processing dependencies for bilm==0.1.post5\n"
          ]
        }
      ],
      "source": [
        "!python setup.py install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwhoSlG5yb7l",
        "outputId": "b102e306-9acc-4de7-e704-95fc896901d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.15`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 1.15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umIIpXnmlwfc"
      },
      "outputs": [],
      "source": [
        "! python bin/restart.py --save_dir ./../checkpoints --vocab_file ./../checkpoints/vocab-2016-09-10.txt --n_gpus 1 --n_epochs 1 --train_prefix \"./../swb/train/*\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3esS_2FBGZw7",
        "outputId": "f052beb4-9cc7-4dc8-b8e5-ba9c6e019a86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 63\n",
            "drwx------ 3 root root  4096 Dec 10 13:07 bilm\n",
            "drwx------ 2 root root  4096 Dec 11 12:49 bilm.egg-info\n",
            "drwx------ 2 root root  4096 Dec 10 13:07 bin\n",
            "drwx------ 4 root root  4096 Dec 10 13:07 build\n",
            "drwx------ 2 root root  4096 Dec 11 12:49 dist\n",
            "-rw------- 1 root root  1165 Oct 16  2018 Dockerfile\n",
            "-rw------- 1 root root    44 Oct 16  2018 .gitignore\n",
            "-rw------- 1 root root 11357 Oct 16  2018 LICENSE\n",
            "-rw------- 1 root root 14724 Oct 16  2018 README.md\n",
            "-rw------- 1 root root   190 Oct 16  2018 run_tests_before_shell.sh\n",
            "-rw------- 1 root root  1738 Oct 16  2018 setup.py\n",
            "drwx------ 2 root root  4096 Dec 10 13:07 tests\n",
            "-rw------- 1 root root  1303 Oct 16  2018 usage_cached.py\n",
            "-rw------- 1 root root  2979 Oct 16  2018 usage_character.py\n",
            "-rw------- 1 root root  3701 Oct 16  2018 usage_token.py\n"
          ]
        }
      ],
      "source": [
        "os.chdir(\"/content/drive/MyDrive/allenmodel\")\n",
        "! ls -al\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2gzh_0cGHtw",
        "outputId": "f6b707d1-e031-4a7e-accc-ed98b8e0cc22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bilm-0.1.post5-py3.7.egg/bilm/training.py:21: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bilm-0.1.post5-py3.7.egg/bilm/training.py:21: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bilm-0.1.post5-py3.7.egg/bilm/training.py:1088: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bilm-0.1.post5-py3.7.egg/bilm/training.py:1089: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2021-12-11 12:49:48.119766: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2021-12-11 12:49:48.120086: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556a47488a00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2021-12-11 12:49:48.120124: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2021-12-11 12:49:48.145269: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2021-12-11 12:49:48.196239: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-12-11 12:49:48.196314: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (e9fcc5f4d227): /proc/driver/nvidia/version does not exist\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bilm-0.1.post5-py3.7.egg/bilm/training.py:1090: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bilm-0.1.post5-py3.7.egg/bilm/training.py:153: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bilm-0.1.post5-py3.7.egg/bilm/training.py:158: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bilm-0.1.post5-py3.7.egg/bilm/training.py:224: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bilm-0.1.post5-py3.7.egg/bilm/training.py:211: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/util/dispatch.py:180: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use the `axis` argument instead\n",
            "USING SKIP CONNECTIONS\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bilm-0.1.post5-py3.7.egg/bilm/training.py:372: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bilm-0.1.post5-py3.7.egg/bilm/training.py:386: The name tf.nn.rnn_cell.ResidualWrapper is deprecated. Please use tf.compat.v1.nn.rnn_cell.ResidualWrapper instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bilm-0.1.post5-py3.7.egg/bilm/training.py:396: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bilm-0.1.post5-py3.7.egg/bilm/training.py:410: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bilm-0.1.post5-py3.7.egg/bilm/training.py:425: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bilm-0.1.post5-py3.7.egg/bilm/training.py:1093: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "INFO:tensorflow:Restoring parameters from ./../checkpoints/model.ckpt-935588\n",
            "2021-12-11 12:49:50.449239: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 1625028608 exceeds 10% of system memory.\n",
            "tcmalloc: large alloc 1625030656 bytes == 0x556a4c8e4000 @  0x7fcc8d840b6b 0x7fcc8d860379 0x7fcc43fc1467 0x7fcc43daec4f 0x7fcc43c7427b 0x7fcc43c39c66 0x7fcc43c3aaf3 0x7fcc43c3acc7 0x7fcc48945d08 0x7fcc489462e2 0x7fcc43fa33a9 0x7fcc43fa0a78 0x7fcc8c1406df 0x7fcc8d6136db 0x7fcc8c74871f\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bilm-0.1.post5-py3.7.egg/bilm/training.py:1097: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "Saving variable lm/char_embed:0 with name char_embed\n",
            "Saving variable lm/CNN/W_cnn_0:0 with name CNN/W_cnn_0\n",
            "Saving variable lm/CNN/b_cnn_0:0 with name CNN/b_cnn_0\n",
            "Saving variable lm/CNN/W_cnn_1:0 with name CNN/W_cnn_1\n",
            "Saving variable lm/CNN/b_cnn_1:0 with name CNN/b_cnn_1\n",
            "Saving variable lm/CNN/W_cnn_2:0 with name CNN/W_cnn_2\n",
            "Saving variable lm/CNN/b_cnn_2:0 with name CNN/b_cnn_2\n",
            "Saving variable lm/CNN/W_cnn_3:0 with name CNN/W_cnn_3\n",
            "Saving variable lm/CNN/b_cnn_3:0 with name CNN/b_cnn_3\n",
            "Saving variable lm/CNN/W_cnn_4:0 with name CNN/W_cnn_4\n",
            "Saving variable lm/CNN/b_cnn_4:0 with name CNN/b_cnn_4\n",
            "Saving variable lm/CNN/W_cnn_5:0 with name CNN/W_cnn_5\n",
            "Saving variable lm/CNN/b_cnn_5:0 with name CNN/b_cnn_5\n",
            "Saving variable lm/CNN/W_cnn_6:0 with name CNN/W_cnn_6\n",
            "Saving variable lm/CNN/b_cnn_6:0 with name CNN/b_cnn_6\n",
            "Saving variable lm/CNN_proj/W_proj:0 with name CNN_proj/W_proj\n",
            "Saving variable lm/CNN_proj/b_proj:0 with name CNN_proj/b_proj\n",
            "Saving variable lm/CNN_high_0/W_carry:0 with name CNN_high_0/W_carry\n",
            "Saving variable lm/CNN_high_0/b_carry:0 with name CNN_high_0/b_carry\n",
            "Saving variable lm/CNN_high_0/W_transform:0 with name CNN_high_0/W_transform\n",
            "Saving variable lm/CNN_high_0/b_transform:0 with name CNN_high_0/b_transform\n",
            "Saving variable lm/CNN_high_1/W_carry:0 with name CNN_high_1/W_carry\n",
            "Saving variable lm/CNN_high_1/b_carry:0 with name CNN_high_1/b_carry\n",
            "Saving variable lm/CNN_high_1/W_transform:0 with name CNN_high_1/W_transform\n",
            "Saving variable lm/CNN_high_1/b_transform:0 with name CNN_high_1/b_transform\n",
            "Saving variable lm/RNN_0/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0 with name RNN_0/RNN/MultiRNNCell/Cell0/LSTMCell/W_0\n",
            "Saving variable lm/RNN_0/rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0 with name RNN_0/RNN/MultiRNNCell/Cell0/LSTMCell/B\n",
            "Saving variable lm/RNN_0/rnn/multi_rnn_cell/cell_0/lstm_cell/projection/kernel:0 with name RNN_0/RNN/MultiRNNCell/Cell0/LSTMCell/W_P_0\n",
            "Saving variable lm/RNN_0/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel:0 with name RNN_0/RNN/MultiRNNCell/Cell1/LSTMCell/W_0\n",
            "Saving variable lm/RNN_0/rnn/multi_rnn_cell/cell_1/lstm_cell/bias:0 with name RNN_0/RNN/MultiRNNCell/Cell1/LSTMCell/B\n",
            "Saving variable lm/RNN_0/rnn/multi_rnn_cell/cell_1/lstm_cell/projection/kernel:0 with name RNN_0/RNN/MultiRNNCell/Cell1/LSTMCell/W_P_0\n",
            "Saving variable lm/RNN_1/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0 with name RNN_1/RNN/MultiRNNCell/Cell0/LSTMCell/W_0\n",
            "Saving variable lm/RNN_1/rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0 with name RNN_1/RNN/MultiRNNCell/Cell0/LSTMCell/B\n",
            "Saving variable lm/RNN_1/rnn/multi_rnn_cell/cell_0/lstm_cell/projection/kernel:0 with name RNN_1/RNN/MultiRNNCell/Cell0/LSTMCell/W_P_0\n",
            "Saving variable lm/RNN_1/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel:0 with name RNN_1/RNN/MultiRNNCell/Cell1/LSTMCell/W_0\n",
            "Saving variable lm/RNN_1/rnn/multi_rnn_cell/cell_1/lstm_cell/bias:0 with name RNN_1/RNN/MultiRNNCell/Cell1/LSTMCell/B\n",
            "Saving variable lm/RNN_1/rnn/multi_rnn_cell/cell_1/lstm_cell/projection/kernel:0 with name RNN_1/RNN/MultiRNNCell/Cell1/LSTMCell/W_P_0\n"
          ]
        }
      ],
      "source": [
        "! python bin/dump_weights.py --save_dir './../checkpoints' --outfile './../swb/swb_weights.hdf5'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "cfRXUdHVLbDy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy.spatial.distance as ds\n",
        "from bilm import Batcher, BidirectionalLanguageModel, weight_layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "p5TXvEbZSlWO"
      },
      "outputs": [],
      "source": [
        "os.chdir(\"/content/drive/MyDrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3ZScv-xQRO0",
        "outputId": "a0a21314-7cc4-461d-d491-a41651887eaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/drive/My Drive/allenmodel/bilm/model.py:276: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/allenmodel/bilm/model.py:333: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/allenmodel/bilm/model.py:378: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/util/dispatch.py:180: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use the `axis` argument instead\n",
            "USING SKIP CONNECTIONS\n",
            "WARNING:tensorflow:From /content/drive/My Drive/allenmodel/bilm/model.py:524: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /content/drive/My Drive/allenmodel/bilm/model.py:568: The name tf.nn.rnn_cell.LSTMStateTuple is deprecated. Please use tf.compat.v1.nn.rnn_cell.LSTMStateTuple instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/allenmodel/bilm/model.py:569: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/drive/My Drive/allenmodel/bilm/model.py:593: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/allenmodel/bilm/model.py:538: The name tf.nn.rnn_cell.ResidualWrapper is deprecated. Please use tf.compat.v1.nn.rnn_cell.ResidualWrapper instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vocab_file = './checkpoints/vocabulary.txt'\n",
        "options_file = './checkpoints/options.json'\n",
        "weight_file = './swb/swb_weights.hdf5'\n",
        "batcher = Batcher(vocab_file, 50)\n",
        "context_character_ids = tf.placeholder('int32', shape=(None, None, 50))\n",
        "bilm = BidirectionalLanguageModel(options_file, weight_file)\n",
        "context_embeddings_op = bilm(context_character_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTYXDEGYbkJO",
        "outputId": "2094eb30-f455-49c5-e744-83a61409bf0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/drive/My Drive/allenmodel/bilm/elmo.py:94: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/allenmodel/bilm/elmo.py:95: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "elmo_context_input = weight_layers('input', context_embeddings_op, l2_coef=0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfP8g0FXoSTD",
        "outputId": "1a796326-7880-40d0-9ab2-bef05db59d8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['Terrifying', 'walls', 'of', 'water', 'from', 'Hurricanes', 'Harvey', 'and', 'Irma,', 'which,', 'when', 'the', 'damage', 'is', 'totaled,', 'could', 'rise', 'to', 'a', 'half', 'trillion', 'dollars.']]\n",
            "<class 'list'>\n"
          ]
        }
      ],
      "source": [
        "raw_context = ['Terrifying walls of water from Hurricanes Harvey and Irma, which, when the damage is totaled, could rise to a half trillion dollars.']\n",
        " \n",
        "tokenized_context = [sentence.split() for sentence in raw_context]\n",
        "print(tokenized_context)\n",
        "\n",
        "print(type(tokenized_context))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nogU53QT2o20",
        "outputId": "99636f2c-7efc-4823-d591-b0b8da1b803f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of context ids =  (1, 24, 50)\n",
            "Shape of generated embeddings =  (1, 22, 1024)\n"
          ]
        }
      ],
      "source": [
        "with tf.Session() as sess:\n",
        "    # It is necessary to initialize variables once before running inference.\n",
        "    sess.run(tf.global_variables_initializer())\n",
        " \n",
        "    # Create batches of data.\n",
        "    context_ids = batcher.batch_sentences(tokenized_context)\n",
        "    print(\"Shape of context ids = \", context_ids.shape)\n",
        " \n",
        "    # Compute ELMo representations (here for the input only, for simplicity).\n",
        "    elmo_context_input_ = sess.run(\n",
        "        elmo_context_input['weighted_op'],\n",
        "        feed_dict={context_character_ids: context_ids}\n",
        "    )\n",
        " \n",
        "    print(\"Shape of generated embeddings = \",elmo_context_input_.shape)\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "RruSgpc_xsQu"
      },
      "outputs": [],
      "source": [
        "def calc_embeddings(tokenized_string):\n",
        "  with tf.Session() as sess:\n",
        "    # It is necessary to initialize variables once before running inference.\n",
        "    sess.run(tf.global_variables_initializer())\n",
        " \n",
        "    # Create batches of data.\n",
        "    context_ids = batcher.batch_sentences(tokenized_string)\n",
        "    print(\"Shape of context ids = \", context_ids.shape)\n",
        " \n",
        "    # Compute ELMo representations (here for the input only, for simplicity).\n",
        "    elmo_context_input_ = sess.run(\n",
        "        elmo_context_input['weighted_op'],\n",
        "        feed_dict={context_character_ids: context_ids}\n",
        "    )\n",
        " \n",
        "    print(\"Shape of generated embeddings = \",elmo_context_input_.shape)\n",
        "\n",
        "    return elmo_context_input_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LP27G3eRwzmr",
        "outputId": "2a73c433-348d-45d5-dbd4-7ab0012f6325"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ],
      "source": [
        "print(type(data))\n",
        "\n",
        "training_data = data[0:200]\n",
        "training_labels = labels[0:200]\n",
        "\n",
        "testing_data = data[200:250]\n",
        "testing_labels = labels[200:250]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVq3uAbHxvIS",
        "outputId": "36a1a4d9-157f-4ae2-96ee-380748a97cd2"
      },
      "outputs": [],
      "source": [
        "train_X = []\n",
        "train_Y = []\n",
        "\n",
        "sum = 0\n",
        "for i in range(len(training_data)):\n",
        "  sentences = []\n",
        "  for j in range(len(training_data[i])):\n",
        "\n",
        "    if training_data[i][j] is None:\n",
        "      continue\n",
        "    if(len(training_data[i][j])<3):\n",
        "      continue\n",
        "\n",
        "    # print(data[i][j])\n",
        "    lst = sent_tokenize(training_data[i][j])\n",
        "\n",
        "    for k in range(len(lst)):\n",
        "      sentences.append(lst[k])\n",
        "\n",
        "  tokenized_context_send = []\n",
        "\n",
        "  for j in range(len(sentences)):\n",
        "\n",
        "    raw_context = sentences[j]\n",
        "    tokenized_context1 = raw_context.split()\n",
        "\n",
        "    new_list = []\n",
        "    for word in tokenized_context1:\n",
        "      word1 = word.lower()\n",
        "\n",
        "      new_word = \"\"\n",
        "\n",
        "      for p in word1:\n",
        "        if (p.isalpha()):\n",
        "          new_word+=p\n",
        "\n",
        "      if(len(new_word)>2 and new_word!=\"the\"):\n",
        "        new_list.append(new_word)\n",
        "\n",
        "    if(len(new_list)>1):\n",
        "      tokenized_context_send.append(new_list)\n",
        "\n",
        "  # print(tokenized_context_send)\n",
        "  \n",
        "  if(len(tokenized_context_send)>=130):\n",
        "    print(tokenized_context_send)\n",
        "    continue\n",
        "  \n",
        "  if(len(tokenized_context_send)<1):\n",
        "    continue\n",
        "\n",
        "  embeddings = calc_embeddings(tokenized_context_send)\n",
        "\n",
        "  feature_vector = []\n",
        "  for j in range(len(tokenized_context_send)):\n",
        "    matrix = []\n",
        "    tot_sum = 0\n",
        "    length_tot = len(tokenized_context_send[j])\n",
        "\n",
        "    for k in range(len(tokenized_context_send[j])):\n",
        "\n",
        "      sm = 0\n",
        "      for p in embeddings[j,k,:]:\n",
        "        sm = sm+p\n",
        "\n",
        "      length = len(embeddings[j,k,:])\n",
        "\n",
        "      num = sm/length\n",
        "      tot_sum+=num\n",
        "    \n",
        "    final_val = tot_sum/length_tot\n",
        "    feature_vector.append(final_val)\n",
        "    \n",
        "\n",
        "  print(feature_vector)\n",
        "  print(labels[i])\n",
        "\n",
        "  train_X.append(feature_vector)\n",
        "  train_Y.append(training_labels[i])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cW1SScGMfxaI",
        "outputId": "4bf659b5-a264-4a16-bc40-902fb1d93759"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "197\n"
          ]
        }
      ],
      "source": [
        "print(len(train_X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qj7-2QzOuA5",
        "outputId": "e4befd0a-8e96-4258-dd5d-f68d4a8f9039"
      },
      "outputs": [],
      "source": [
        "test_X = []\n",
        "test_Y = []\n",
        "\n",
        "sum = 0\n",
        "for i in range(len(testing_data)):\n",
        "  sentences = []\n",
        "  for j in range(len(testing_data[i])):\n",
        "\n",
        "    if testing_data[i][j] is None:\n",
        "      continue\n",
        "    if(len(testing_data[i][j])<3):\n",
        "      continue\n",
        "\n",
        "    # print(data[i][j])\n",
        "    lst = sent_tokenize(testing_data[i][j])\n",
        "\n",
        "    for k in range(len(lst)):\n",
        "      sentences.append(lst[k])\n",
        "\n",
        "  tokenized_context_send = []\n",
        "\n",
        "  for j in range(len(sentences)):\n",
        "\n",
        "    raw_context = sentences[j]\n",
        "    tokenized_context1 = raw_context.split()\n",
        "\n",
        "    new_list = []\n",
        "    for word in tokenized_context1:\n",
        "      word1 = word.lower()\n",
        "\n",
        "      new_word = \"\"\n",
        "\n",
        "      for p in word1:\n",
        "        if (p.isalpha()):\n",
        "          new_word+=p\n",
        "\n",
        "      if(len(new_word)>2 and new_word!=\"the\"):\n",
        "        new_list.append(new_word)\n",
        "\n",
        "    if(len(new_list)>1):\n",
        "      tokenized_context_send.append(new_list)\n",
        "\n",
        "  # print(tokenized_context_send)\n",
        "  \n",
        "  if(len(tokenized_context_send)>=130):\n",
        "    print(tokenized_context_send)\n",
        "    continue\n",
        "  \n",
        "  if(len(tokenized_context_send)>0):\n",
        "    embeddings = calc_embeddings(tokenized_context_send)\n",
        "\n",
        "  feature_vector = []\n",
        "  for j in range(len(tokenized_context_send)):\n",
        "    matrix = []\n",
        "    tot_sum = 0\n",
        "    length_tot = len(tokenized_context_send[j])\n",
        "\n",
        "    for k in range(len(tokenized_context_send[j])):\n",
        "\n",
        "      sm = 0\n",
        "      for p in embeddings[j,k,:]:\n",
        "        sm = sm+p\n",
        "\n",
        "      length = len(embeddings[j,k,:])\n",
        "\n",
        "      num = sm/length\n",
        "      tot_sum+=num\n",
        "    \n",
        "    final_val = tot_sum/length_tot\n",
        "    feature_vector.append(final_val)\n",
        "    \n",
        "\n",
        "  print(feature_vector)\n",
        "  print(testing_labels[i])\n",
        "\n",
        "  test_X.append(feature_vector)\n",
        "  test_Y.append(testing_labels[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xm7u1K1SVfSC",
        "outputId": "ca932763-8a06-4d86-9e1e-792513d2c520"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1]\n",
            "[0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "print(train_Y)\n",
        "print(test_Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0uRB7UiR1Ae",
        "outputId": "6b1e640a-fbd8-49ca-c514-b7a0a0334007"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "train_X1 = pad_sequences(train_X, maxlen=140,dtype='float32')\n",
        "test_X1 = pad_sequences(test_X,maxlen=140,dtype='float32')\n",
        "\n",
        "train_Y1 = np.array(train_Y)\n",
        "test_Y1 = np.array(test_Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "bFGd0KUrWZKQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "QztAcoBWU00H"
      },
      "outputs": [],
      "source": [
        "def bootstrapping(X_train, y_train , n_bootstrap):\n",
        "    bootstrap_indices = np.random.randint(low=0, high=len(X_train), size=n_bootstrap)\n",
        "    df_bootstrapped = X_train[bootstrap_indices]\n",
        "\n",
        "    df2_bootstrapped = y_train[bootstrap_indices]\n",
        "    return df_bootstrapped , df2_bootstrapped\n",
        "\n",
        "def random_forest_algorithm(X_train, y_train , n_trees, n_bootstrap, n_features, dt_max_depth):\n",
        "    forest = []\n",
        "    for i in range(n_trees):\n",
        "        df_bootstrapped , df2_bootstrapped = bootstrapping(X_train,y_train, n_bootstrap)\n",
        "\n",
        "        tree = DecisionTreeClassifier(criterion = \"entropy\",random_state = n_features , max_depth=dt_max_depth)\n",
        "        \n",
        "        tree.fit(df_bootstrapped, df2_bootstrapped)\n",
        "        forest.append(tree)\n",
        "    \n",
        "    return forest\n",
        "\n",
        "def random_forest_predictions(X_test, forest):\n",
        "    df_predictions = {}\n",
        "    for i in range(len(forest)):\n",
        "        column_name = 'predicted'\n",
        "        predictions = forest[i].predict(X_test)\n",
        "        df_predictions[column_name] = predictions\n",
        "\n",
        "    df_predictions = pd.DataFrame(df_predictions)\n",
        "    random_forest_predictions = df_predictions.mode(axis=1)[0]\n",
        "    \n",
        "    return random_forest_predictions\n",
        "\n",
        "def cal_accuracy(y_test , y_pred):\n",
        "  print (\"Accuracy : \",\n",
        "    accuracy_score(y_test,y_pred)*100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrA73upMUeBG",
        "outputId": "36fa4efa-f205-47b5-b165-f0a87b8d02fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy :  75.51020408163265\n"
          ]
        }
      ],
      "source": [
        "forest = random_forest_algorithm(train_X1 , train_Y1 , n_trees=1000, n_bootstrap=100, n_features=6, dt_max_depth=5)\n",
        "predictions = random_forest_predictions(test_X1, forest)\n",
        "cal_accuracy(test_Y1,predictions)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "code.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
