{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S0KeVwv61nYC"
      },
      "outputs": [],
      "source": [
        "import gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXZbyy1i14bf",
        "outputId": "cb8c944b-78e0-4a5b-a453-16da6dadd32c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHVknudtWwWs",
        "outputId": "59ddba22-2dcb-424b-f95e-b8f7fb028230"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyUIgtQG2P1D",
        "outputId": "9ee2972f-b829-480d-f661-921b623c9ed0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hollywood will not easily recover from Harvey Weinstein -- not for a long time. The hypocrisy level has hit Defcon 1, 9.9 on the Richter scale.', \"Hollywood’s politics have always been a self-serving charade, a liberal masquerade for a rapacious and lubricious lifestyle. But now, thanks to the Weinstein scandal, we see it more clearly than ever. And it couldn't be more repellent. (I had always thought Bill Clinton would have made the greatest studio executive of all time. Now I'm convinced of it.)\", 'If conservative investors had any courage, this would be the time to make a hostile takeover of the movie business. Unfortunately, they don’t. I know this from bitter personal experience. Wealthy conservatives are delighted to support the Philharmonic, but when it comes to popular culture they turn away, as if afraid to get their hands dirty.', 'That this is a huge mistake should be obvious. They have abandoned the culture -- and our children -- to the creepiest people imaginable. What is going on in Hollywood is far from being just about Harvey. It’s approaching a pandemic. So many previously silent assaulted or raped women are coming out of the woodwork, it seems like a long-belated remake of “Cheaper by the Dozen.” No one knows who will be next or if it will stop at Harvey.', 'The rot is everywhere, even, perhaps especially, in the precincts of “high art.” Gwyneth Paltrow says now is the time to put an end to these attacks on women. But where was she years ago when Harvey got “handsy” with her? Looking the other way while earning millions and garnering Oscars. Meryl Streep claimed she was clueless about Weinstein’s repulsive antics. Time to award her her greatest Oscar yet -- for playing someone deaf, dumb, and blind while living as a troglodyte in the Gobi desert. Either the woman’s a liar or an utter nincompoop. I’ll go with the former.', 'As for the great feminist George Clooney -- the first male star out of the box to condemn Weinstein’s behavior -- let’s give him the Nobel Prize in virtue signaling. By coming forward, he was able to ace out his competition -- Howard Zinn-loving Matt Damon, who disgraced himself forever by covering up for Harvey a decade ago. (For those who may have missed it in the onslaught of sleazy details, Damon assured then New York Times reporter Sharon Waxman that Miramax’s high-paid Italian representative was a genuine “creative film executive” and not Harvey’s European procurer, as was, evidently correctly, rumored. Damon is the same “progressive” movie star who makes films opposing school choice for the masses while living in a thirty million dollar house and sending his kids to private school. I take it back -- maybe we should give him the Nobel in virtue signaling.)']\n"
          ]
        }
      ],
      "source": [
        "os.chdir(\"/content/drive/MyDrive\")\n",
        "xml_data = open('./dataset/articles-training-byarticle-20181122.xml', 'r').read()  # Read file\n",
        "root = ET.XML(xml_data)\n",
        "\n",
        "data = []\n",
        "\n",
        "for i, child in enumerate(root):\n",
        "    data.append([subchild.text for subchild in child])\n",
        "\n",
        "df = pd.DataFrame(data).T  # Write in DF and transpose it\n",
        "\n",
        "print(data[218])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxNVwrLt2Seg",
        "outputId": "1f2488ce-b8c3-43d7-95f1-0593d2bf64fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n"
          ]
        }
      ],
      "source": [
        "xml_data1 = open('./dataset/ground-truth-training-byarticle-20181122.xml', 'r').read()  # Read file\n",
        "root1 = ET.XML(xml_data1)\n",
        "\n",
        "labels = []\n",
        "\n",
        "for i, child in enumerate(root1):\n",
        "    if child.attrib['hyperpartisan']=='true':\n",
        "        labels.append(1)\n",
        "    else:\n",
        "        labels.append(0)\n",
        "\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "NZBYoIy32W68"
      },
      "outputs": [],
      "source": [
        "training_data = data[0:500]\n",
        "training_labels = labels[0:500]\n",
        "\n",
        "testing_data = data[500:640]\n",
        "testing_labels = labels[500:640]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "E5wjQhr-3FFm"
      },
      "outputs": [],
      "source": [
        "train_X = []\n",
        "train_Y = []\n",
        "\n",
        "sum = 0\n",
        "for i in range(len(training_data)):\n",
        "  sentences = []\n",
        "  for j in range(len(training_data[i])):\n",
        "\n",
        "    if training_data[i][j] is None:\n",
        "      continue\n",
        "    if(len(training_data[i][j])<3):\n",
        "      continue\n",
        "\n",
        "    # print(data[i][j])\n",
        "    lst = sent_tokenize(training_data[i][j])\n",
        "\n",
        "    for k in range(len(lst)):\n",
        "      sentences.append(lst[k])\n",
        "\n",
        "  tokenized_context_send = []\n",
        "\n",
        "  for j in range(len(sentences)):\n",
        "\n",
        "    raw_context = sentences[j]\n",
        "    tokenized_context1 = raw_context.split()\n",
        "\n",
        "    new_list = []\n",
        "    for word in tokenized_context1:\n",
        "      word1 = word.lower()\n",
        "\n",
        "      new_word = \"\"\n",
        "\n",
        "      for p in word1:\n",
        "        if (p.isalpha()):\n",
        "          new_word+=p\n",
        "\n",
        "      if(len(new_word)>2 and new_word!=\"the\"):\n",
        "        new_list.append(new_word)\n",
        "\n",
        "    if(len(new_list)>1):\n",
        "      tokenized_context_send.append(new_list)\n",
        "\n",
        "  # print(tokenized_context_send)\n",
        "  \n",
        "  train_X.append(tokenized_context_send)\n",
        "  train_Y.append(training_labels[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "onxvO21Ae30J"
      },
      "outputs": [],
      "source": [
        "test_X = []\n",
        "test_Y = []\n",
        "\n",
        "sum = 0\n",
        "for i in range(len(testing_data)):\n",
        "  sentences = []\n",
        "  for j in range(len(testing_data[i])):\n",
        "\n",
        "    if testing_data[i][j] is None:\n",
        "      continue\n",
        "    if(len(testing_data[i][j])<3):\n",
        "      continue\n",
        "\n",
        "    # print(data[i][j])\n",
        "    lst = sent_tokenize(testing_data[i][j])\n",
        "\n",
        "    for k in range(len(lst)):\n",
        "      sentences.append(lst[k])\n",
        "\n",
        "  tokenized_context_send = []\n",
        "\n",
        "  for j in range(len(sentences)):\n",
        "\n",
        "    raw_context = sentences[j]\n",
        "    tokenized_context1 = raw_context.split()\n",
        "\n",
        "    new_list = []\n",
        "    for word in tokenized_context1:\n",
        "      word1 = word.lower()\n",
        "\n",
        "      new_word = \"\"\n",
        "\n",
        "      for p in word1:\n",
        "        if (p.isalpha()):\n",
        "          new_word+=p\n",
        "\n",
        "      if(len(new_word)>2 and new_word!=\"the\"):\n",
        "        new_list.append(new_word)\n",
        "\n",
        "    if(len(new_list)>1):\n",
        "      tokenized_context_send.append(new_list)\n",
        "\n",
        "  # print(tokenized_context_send)\n",
        "  \n",
        "  test_X.append(tokenized_context_send)\n",
        "  test_Y.append(testing_labels[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "-_Q37j5fXBbA"
      },
      "outputs": [],
      "source": [
        "sentences = []\n",
        "for i in range(len(train_X)):\n",
        "  for j in range(len(train_X[i])):\n",
        "    sentences.append(train_X[i][j])\n",
        "\n",
        "for i in range(len(test_X)):\n",
        "  for j in range(len(test_X[i])):\n",
        "    sentences.append(test_X[i][j])\n",
        "\n",
        "# print(sentences[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wlu7Uy7Z1v92",
        "outputId": "b8e0d120-35ec-496b-b7e9-0b4136126e7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result embedding shape: (19291, 100)\n",
            "Checking similar words:\n",
            "  model -> aggrandizement (0.48), coal (0.45), earmarked (0.44), patients (0.44), standby (0.44), sensefor (0.44), miner (0.44), careers (0.43)\n",
            "  network -> spokesman (0.52), antidefamation (0.47), abc (0.46), warner (0.45), bureaus (0.45), rulebook (0.45), pitcavage (0.45), cooper (0.43)\n",
            "  train -> destroys (0.63), awayto (0.56), districts (0.54), voids (0.54), moorpark (0.54), burbank (0.53), kitchen (0.50), kan (0.49)\n",
            "  learn -> read (0.43), bothered (0.42), ryanhes (0.41), gripe (0.41), writes (0.41), preach (0.41), dickersons (0.40), toss (0.39)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "word_model = gensim.models.Word2Vec(sentences, size=100 , min_count=1 , window=5, iter=100)\n",
        "pretrained_weights = word_model.wv.syn0\n",
        "vocab_size, emdedding_size = pretrained_weights.shape\n",
        "print('Result embedding shape:', pretrained_weights.shape)\n",
        "print('Checking similar words:')\n",
        "for word in ['model', 'network', 'train', 'learn']:\n",
        "  most_similar = ', '.join('%s (%.2f)' % (similar, dist) \n",
        "                           for similar, dist in word_model.most_similar(word)[:8])\n",
        "  print('  %s -> %s' % (word, most_similar))\n",
        "\n",
        "def word2idx(word):\n",
        "  return word_model.wv.vocab[word].index\n",
        "def idx2word(idx):\n",
        "  return word_model.wv.index2word[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9JBQzgifa4H"
      },
      "outputs": [],
      "source": [
        "print(pretrained_weights[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBMqXY8Ofv0S",
        "outputId": "9d94ce1a-7972-4389-9241-74064b3b7cae"
      },
      "outputs": [],
      "source": [
        "final_train_X = []\n",
        "final_train_Y = []\n",
        "for i in range(len(train_X)):\n",
        "  feature_vector = []\n",
        "  for j in range(len(train_X[i])):\n",
        "    \n",
        "    sm = 0\n",
        "    length = len(train_X[i][j])\n",
        "    for words in train_X[i][j]:\n",
        "      weight = pretrained_weights[word2idx(words)]\n",
        "    \n",
        "      sm1 = 0\n",
        "      for k in range(len(weight)):\n",
        "        sm1=sm1+weight[k]\n",
        "\n",
        "      sm = sm + sm1/len(weight)\n",
        "\n",
        "    feature_vector.append(sm)\n",
        "\n",
        "  print(feature_vector)\n",
        "  final_train_X.append(feature_vector)\n",
        "  final_train_Y.append(train_Y[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "mjFH2NpLllnP"
      },
      "outputs": [],
      "source": [
        "final_test_X = []\n",
        "final_test_Y = []\n",
        "for i in range(len(test_X)):\n",
        "  feature_vector = []\n",
        "  for j in range(len(test_X[i])):\n",
        "    \n",
        "    sm = 0\n",
        "    length = len(test_X[i][j])\n",
        "    for words in test_X[i][j]:\n",
        "      weight = pretrained_weights[word2idx(words)]\n",
        "    \n",
        "      sm1 = 0\n",
        "      for k in range(len(weight)):\n",
        "        sm1=sm1+weight[k]\n",
        "\n",
        "      sm = sm + sm1/len(weight)\n",
        "\n",
        "    feature_vector.append(sm)\n",
        "\n",
        "  final_test_X.append(feature_vector)\n",
        "  final_test_Y.append(test_Y[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qzuNPilmFvq",
        "outputId": "c4e90af1-f93a-4a86-ea30-dd0968bc8024"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "60\n",
            "[1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# print(np.array(final_train_X).shape)\n",
        "print(len(final_train_X[0]))\n",
        "print(final_train_Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "str7dvy5ZBji"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "train_Xp = pad_sequences( final_train_X , maxlen=100 , dtype='float32')\n",
        "test_Xp = pad_sequences( final_test_X , maxlen=100 , dtype='float32')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "kRkd0-0EmmmY"
      },
      "outputs": [],
      "source": [
        "from keras import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout , Activation , Bidirectional\n",
        "from keras.layers import SimpleRNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "DAzrGLms7Qqv"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "for i in range(len(train_Xp)):\n",
        "  train_Xp[i] = softmax(train_Xp[i])\n",
        "\n",
        "for i in range(len(test_Xp)):\n",
        "  test_Xp[i] = softmax(test_Xp[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZL2KNqq1wyy"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "\n",
        "\n",
        "model.add(Embedding(input_dim=vocab_size,\n",
        "                    output_dim=emdedding_size,\n",
        "                    weights=[pretrained_weights],\n",
        "                    input_length=100,\n",
        "                    mask_zero=True,\n",
        "                    trainable=False))\n",
        "\n",
        "model.add(Bidirectional(LSTM(100)))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "train_X1 = np.asarray(train_Xp)\n",
        "train_Y1 = np.asarray(final_train_Y)\n",
        "\n",
        "print(train_X1)\n",
        "print(train_Y1)\n",
        "\n",
        "history = model.fit(train_X1, train_Y1, epochs=5, batch_size=64, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8py1N9hm_1R",
        "outputId": "021ee699-b264-4dbc-9636-5b9e0ef676c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7285714149475098\n"
          ]
        }
      ],
      "source": [
        "test_X1 = np.asarray(test_Xp)\n",
        "test_Y1 = np.asarray(final_test_Y)\n",
        "\n",
        "\n",
        "scores = model.evaluate(test_X1, test_Y1, verbose=0)\n",
        "print(scores[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "6OTKGGGjO6rN"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "model1 = SVC(C=0.1 , gamma='scale' , kernel='rbf')\n",
        "\n",
        "model1.fit(train_X1,train_Y1)\n",
        "y1_predict = model1.predict(test_X1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQi-2zfDPa9A",
        "outputId": "6d108173-fec4-4a19-ee89-8be32e4ec8d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      1.00      0.84       102\n",
            "           1       0.00      0.00      0.00        38\n",
            "\n",
            "    accuracy                           0.73       140\n",
            "   macro avg       0.36      0.50      0.42       140\n",
            "weighted avg       0.53      0.73      0.61       140\n",
            "\n",
            "-----------------------------------------------------------\n",
            "Accuracy: 0.73\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(classification_report(test_Y1, y1_predict))\n",
        "print('-----------------------------------------------------------')\n",
        "print(f\"Accuracy: {round(accuracy_score(test_Y1, y1_predict), 2)}\") "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Lstm.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
